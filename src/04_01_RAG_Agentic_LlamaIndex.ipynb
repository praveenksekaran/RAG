{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agentic RAG with Llamaindex and LLM\n",
    "\n",
    "#### 1. pip install necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fastembed llama-index-vector-stores-chroma llama-index-llms-mistralai llama-index-embeddings-fastembed llama-index-readers-file\n",
    "!pip install llama_index.vector_stores.chroma llama_index.vector_stores.chroma llama_index.llms.mistralai llama_index.embeddings.fastembed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Import necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "import chromadb\n",
    "from llama_index.core import SimpleDirectoryReader,VectorStoreIndex,SummaryIndex\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.core.agent import FunctionCallingAgentWorker\n",
    "from llama_index.core.vector_stores import MetadataFilters,FilterCondition\n",
    "from llama_index.core.agent import AgentRunner\n",
    "from llama_index.core.objects import ObjectIndex\n",
    "from llama_index.core.tools import FunctionTool\n",
    "from llama_index.llms.mistralai import MistralAI\n",
    "from typing import List,Optional\n",
    "import os\n",
    "#from llama_index.embeddings.fastembed import FastEmbedEmbedding\n",
    "from llama_index.core import Settings\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Download Dataset\n",
    "\n",
    "urls = [\n",
    "    \"https://openreview.net/pdf?id=VtmBAGCN7o\",\n",
    "    \"https://openreview.net/pdf?id=6PmJoRfdaK\",\n",
    "    \"https://openreview.net/pdf?id=hSyW5go0v8\",\n",
    "]\n",
    "\n",
    "papers = [\n",
    "    \"metagpt.pdf\",\n",
    "    \"longlora.pdf\",\n",
    "    \"selfrag.pdf\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file(url, file_path):\n",
    "    \"\"\"Downloads a file from a given URL and saves it to the specified file path.\n",
    "\n",
    "    Args:\n",
    "        url: The URL of the file to download.\n",
    "        file_path: The path to save the downloaded file.\n",
    "    \"\"\"\n",
    "\n",
    "    response = requests.get(url, stream=True)\n",
    "    response.raise_for_status()  # Raise an exception for non-200 status codes\n",
    "\n",
    "    with open(file_path, \"wb\") as f:\n",
    "        for chunk in response.iter_content(chunk_size=1024):\n",
    "            if chunk:  # Filter out keep-alive new chunks\n",
    "                f.write(chunk)\n",
    "\n",
    "    print(f\"Downloaded file from {url} to {file_path}\")\n",
    "\n",
    "\n",
    "for url, paper in zip(urls, papers):\n",
    "    download_file(url, paper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4 . Store vectos in chroma db\n",
    "\n",
    "The object index supports integrations with any existing storage backend in LlamaIndex.\n",
    "\n",
    "The following section walks through how to set that up using Chroma db as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "chroma_collection = db.get_or_create_collection(\"multidocument-agent\")\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5 . Define the model with API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"MISTRAL_API_KEY\"] = \"\"\n",
    "Settings.llm = MistralAI(model=\"mistral-large-latest\")\n",
    "Settings.embed_model = FastEmbedEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6 . Function to Create Summary tool and vector Tool\n",
    "\n",
    "For each document, we set up two types of indices:\n",
    "\n",
    "- Vector Index: This is used for semantic search, which means it helps in understanding and retrieving information based on the meaning and context of the query.\n",
    "- Summary Index: This is used for generating summaries of the document, providing a concise overview of the content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: abstract all of this into a function that takes in a PDF file name\n",
    "def get_doc_tools(\n",
    "    file_path: str,\n",
    "    name: str,\n",
    ") -> str:\n",
    "    \"\"\"Get vector query and summary query tools from a document.\"\"\"\n",
    "\n",
    "    # load documents\n",
    "    documents = SimpleDirectoryReader(input_files=[file_path]).load_data()\n",
    "    splitter = SentenceSplitter(chunk_size=1024)\n",
    "    nodes = splitter.get_nodes_from_documents(documents)\n",
    "    vector_index = VectorStoreIndex.from_documents(\n",
    "        documents, storage_context=storage_context\n",
    "    )\n",
    "    summary_index = SummaryIndex(nodes)\n",
    "\n",
    "    def vector_query(\n",
    "        query: str, page_numbers: Optional[List[str]] = None\n",
    "    ) -> str:\n",
    "        \"\"\"Use to answer questions over the MetaGPT paper.\n",
    "\n",
    "        Useful if you have specific questions over the MetaGPT paper.\n",
    "        Always leave page_numbers as None UNLESS there is a specific page you want to search for.\n",
    "\n",
    "        Args:\n",
    "            query (str): the string query to be embedded.\n",
    "            page_numbers (Optional[List[str]]): Filter by set of pages. Leave as NONE\n",
    "                if we want to perform a vector search\n",
    "                over all pages. Otherwise, filter by the set of specified pages.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        page_numbers = page_numbers or []\n",
    "        metadata_dicts = [\n",
    "            {\"key\": \"page_label\", \"value\": p} for p in page_numbers\n",
    "        ]\n",
    "\n",
    "        query_engine = vector_index.as_query_engine(\n",
    "            similarity_top_k=2,\n",
    "            filters=MetadataFilters.from_dicts(\n",
    "                metadata_dicts, condition=FilterCondition.OR\n",
    "            ),\n",
    "        )\n",
    "        response = query_engine.query(query)\n",
    "        return response\n",
    "\n",
    "    vector_query_tool = FunctionTool.from_defaults(\n",
    "        name=f\"vector_tool_{name}\", fn=vector_query\n",
    "    )\n",
    "\n",
    "    def summary_query(\n",
    "        query: str,\n",
    "    ) -> str:\n",
    "        \"\"\"Perform a summary of document\n",
    "        query (str): the string query to be embedded.\n",
    "        \"\"\"\n",
    "        summary_engine = summary_index.as_query_engine(\n",
    "            response_mode=\"tree_summarize\",\n",
    "            use_async=True,\n",
    "        )\n",
    "\n",
    "        response = summary_engine.query(query)\n",
    "        return response\n",
    "\n",
    "    summary_tool = FunctionTool.from_defaults(\n",
    "        fn=summary_query, name=f\"summary_tool_{name}\"\n",
    "    )\n",
    "\n",
    "    return vector_query_tool, summary_tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7 . Index the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_to_tools_dict = {}\n",
    "papers = [\n",
    "    \"metagpt.pdf\",\n",
    "    \"longlora.pdf\",\n",
    "    \"selfrag.pdf\",\n",
    "]\n",
    "for paper in papers:\n",
    "    print(f\"Getting tools for paper: {paper}\")\n",
    "    vector_tool, summary_tool = get_doc_tools(paper, Path(paper).stem)\n",
    "    paper_to_tools_dict[paper] = [vector_tool, summary_tool]\n",
    "all_tools = [t for paper in papers for t in paper_to_tools_dict[paper]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ObjectIndex class is designed to enable the indexing of various Python objects, making it highly versatile for numerous applications.\n",
    "\n",
    "To create an ObjectIndex, you need two components: the index itself and a ObjectNodeMapping. The ObjectNodeMapping serves as a bridge, allowing you to map between a node and its corresponding object, and vice versa. Alternatively, the ObjectIndex class provides a from_objects() class method that simplifies the creation process by allowing you to construct an ObjectIndex directly from a collection of objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_index = ObjectIndex.from_objects(\n",
    "    all_tools,\n",
    "    index_cls=VectorStoreIndex,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, you can enhance the functionality of an ObjectIndex retriever by adding node-postprocessors. These postprocessors, such as rerankers, provide convenient ways to further process or refine the retrieval results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_retriever = obj_index.as_retriever(similarity_top_k=3)\n",
    "tools = obj_retriever.retrieve(\"compare and contrast the papers self rag and metagpt\")\n",
    "#\n",
    "print(tools[0].metadata)\n",
    "print(tools[1].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Use our Mistral agent, powered by function calling capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Agent Runner\n",
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    [vector_query_tool, summary_tool], llm=Settings.llm, verbose=True\n",
    ")\n",
    "agent = AgentRunner(agent_worker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9 . Test agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = agent.query(\n",
    "    \"what are agent roles in MetaGPT, \"\n",
    "    \"and then how they communicate with each other.\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
