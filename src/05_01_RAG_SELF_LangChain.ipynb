{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a01317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install langchain langchain-openai python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a861de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Praveen\\Projects\\RAG\\venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3672: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "from common_functions import *\n",
    "\n",
    "# Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Set the OpenAI API key environment variable\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb894f6",
   "metadata": {},
   "source": [
    "#### Download required data files\n",
    "import os\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "#### Download the PDF document used in this notebook\n",
    "!wget -O data/Understanding_Climate_Change.pdf https://raw.githubusercontent.com/NirDiamant/RAG_TECHNIQUES/main/data/Understanding_Climate_Change.pdf\n",
    "!wget -O data/Understanding_Climate_Change.pdf https://raw.githubusercontent.com/NirDiamant/RAG_TECHNIQUES/main/data/Understanding_Climate_Change.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63e8334",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"data/Understanding_Climate_Change.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d389ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = encode_pdf(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f58fecc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", max_tokens=1000, temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accb5361",
   "metadata": {},
   "source": [
    "#### Defining prompt templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f64d71bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Praveen\\Projects\\RAG\\venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1844: UserWarning: Received a Pydantic BaseModel V1 schema. This is not supported by method=\"json_schema\". Please use method=\"function_calling\" or specify schema via JSON Schema or Pydantic V2 BaseModel. Overriding to method=\"function_calling\".\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "class RetrievalResponse(BaseModel):\n",
    "    response: str = Field(..., title=\"Determines if retrieval is necessary\", description=\"Output only 'Yes' or 'No'.\")\n",
    "retrieval_prompt = PromptTemplate(\n",
    "    input_variables=[\"query\"],\n",
    "    template=\"Given the query '{query}', determine if retrieval is necessary. Output only 'Yes' or 'No'.\"\n",
    ")\n",
    "\n",
    "class RelevanceResponse(BaseModel):\n",
    "    response: str = Field(..., title=\"Determines if context is relevant\", description=\"Output only 'Relevant' or 'Irrelevant'.\")\n",
    "relevance_prompt = PromptTemplate(\n",
    "    input_variables=[\"query\", \"context\"],\n",
    "    template=\"Given the query '{query}' and the context '{context}', determine if the context is relevant. Output only 'Relevant' or 'Irrelevant'.\"\n",
    ")\n",
    "\n",
    "class GenerationResponse(BaseModel):\n",
    "    response: str = Field(..., title=\"Generated response\", description=\"The generated response.\")\n",
    "generation_prompt = PromptTemplate(\n",
    "    input_variables=[\"query\", \"context\"],\n",
    "    template=\"Given the query '{query}' and the context '{context}', generate a response.\"\n",
    ")\n",
    "\n",
    "class SupportResponse(BaseModel):\n",
    "    response: str = Field(..., title=\"Determines if response is supported\", description=\"Output 'Fully supported', 'Partially supported', or 'No support'.\")\n",
    "support_prompt = PromptTemplate(\n",
    "    input_variables=[\"response\", \"context\"],\n",
    "    template=\"Given the response '{response}' and the context '{context}', determine if the response is supported by the context. Output 'Fully supported', 'Partially supported', or 'No support'.\"\n",
    ")\n",
    "\n",
    "class UtilityResponse(BaseModel):\n",
    "    response: int = Field(..., title=\"Utility rating\", description=\"Rate the utility of the response from 1 to 5.\")\n",
    "utility_prompt = PromptTemplate(\n",
    "    input_variables=[\"query\", \"response\"],\n",
    "    template=\"Given the query '{query}' and the response '{response}', rate the utility of the response from 1 to 5.\"\n",
    ")\n",
    "\n",
    "# Create LLMChains for each step\n",
    "retrieval_chain = retrieval_prompt | llm.with_structured_output(RetrievalResponse)\n",
    "relevance_chain = relevance_prompt | llm.with_structured_output(RelevanceResponse)\n",
    "generation_chain = generation_prompt | llm.with_structured_output(GenerationResponse)\n",
    "support_chain = support_prompt | llm.with_structured_output(SupportResponse)\n",
    "utility_chain = utility_prompt | llm.with_structured_output(UtilityResponse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc7432b",
   "metadata": {},
   "source": [
    "#### Defining the self RAG logic flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b973e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_rag(query, vectorstore, top_k=3):\n",
    "    print(f\"\\nProcessing query: {query}\")\n",
    "    \n",
    "    # Step 1: Determine if retrieval is necessary\n",
    "    print(\"Step 1: Determining if retrieval is necessary...\")\n",
    "    input_data = {\"query\": query}\n",
    "    retrieval_decision = retrieval_chain.invoke(input_data).response.strip().lower()\n",
    "    print(f\"Retrieval decision: {retrieval_decision}\")\n",
    "    \n",
    "    if retrieval_decision == 'yes':\n",
    "        # Step 2: Retrieve relevant documents\n",
    "        print(\"Step 2: Retrieving relevant documents...\")\n",
    "        docs = vectorstore.similarity_search(query, k=top_k)\n",
    "        contexts = [doc.page_content for doc in docs]\n",
    "        print(f\"Retrieved {len(contexts)} documents\")\n",
    "        \n",
    "        # Step 3: Evaluate relevance of retrieved documents\n",
    "        print(\"Step 3: Evaluating relevance of retrieved documents...\")\n",
    "        relevant_contexts = []\n",
    "        for i, context in enumerate(contexts):\n",
    "            input_data = {\"query\": query, \"context\": context}\n",
    "            relevance = relevance_chain.invoke(input_data).response.strip().lower()\n",
    "            print(f\"Document {i+1} relevance: {relevance}\")\n",
    "            if relevance == 'relevant':\n",
    "                relevant_contexts.append(context)\n",
    "        \n",
    "        print(f\"Number of relevant contexts: {len(relevant_contexts)}\")\n",
    "        \n",
    "        # If no relevant contexts found, generate without retrieval\n",
    "        if not relevant_contexts:\n",
    "            print(\"No relevant contexts found. Generating without retrieval...\")\n",
    "            input_data = {\"query\": query, \"context\": \"No relevant context found.\"}\n",
    "            return generation_chain.invoke(input_data).response\n",
    "        \n",
    "        # Step 4: Generate response using relevant contexts\n",
    "        print(\"Step 4: Generating responses using relevant contexts...\")\n",
    "        responses = []\n",
    "        for i, context in enumerate(relevant_contexts):\n",
    "            print(f\"Generating response for context {i+1}...\")\n",
    "            input_data = {\"query\": query, \"context\": context}\n",
    "            response = generation_chain.invoke(input_data).response\n",
    "            \n",
    "            # Step 5: Assess support\n",
    "            print(f\"Step 5: Assessing support for response {i+1}...\")\n",
    "            input_data = {\"response\": response, \"context\": context}\n",
    "            support = support_chain.invoke(input_data).response.strip().lower()\n",
    "            print(f\"Support assessment: {support}\")\n",
    "            \n",
    "            # Step 6: Evaluate utility\n",
    "            print(f\"Step 6: Evaluating utility for response {i+1}...\")\n",
    "            input_data = {\"query\": query, \"response\": response}\n",
    "            utility = int(utility_chain.invoke(input_data).response)\n",
    "            print(f\"Utility score: {utility}\")\n",
    "            \n",
    "            responses.append((response, support, utility))\n",
    "        \n",
    "        # Select the best response based on support and utility\n",
    "        print(\"Selecting the best response...\")\n",
    "        best_response = max(responses, key=lambda x: (x[1] == 'fully supported', x[2]))\n",
    "        print(f\"Best response support: {best_response[1]}, utility: {best_response[2]}\")\n",
    "        return best_response[0]\n",
    "    else:\n",
    "        # Generate without retrieval\n",
    "        print(\"Generating without retrieval...\")\n",
    "        input_data = {\"query\": query, \"context\": \"No retrieval necessary.\"}\n",
    "        return generation_chain.invoke(input_data).response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76496abb",
   "metadata": {},
   "source": [
    "#### Test the self-RAG function easy query with high relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f22cf4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing query: What is the impact of climate change on the environment?\n",
      "Step 1: Determining if retrieval is necessary...\n",
      "Retrieval decision: yes\n",
      "Step 2: Retrieving relevant documents...\n",
      "Retrieved 3 documents\n",
      "Step 3: Evaluating relevance of retrieved documents...\n",
      "Document 1 relevance: relevant\n",
      "Document 2 relevance: relevant\n",
      "Document 3 relevance: relevant\n",
      "Number of relevant contexts: 3\n",
      "Step 4: Generating responses using relevant contexts...\n",
      "Generating response for context 1...\n",
      "Step 5: Assessing support for response 1...\n",
      "Support assessment: fully supported\n",
      "Step 6: Evaluating utility for response 1...\n",
      "Utility score: 5\n",
      "Generating response for context 2...\n",
      "Step 5: Assessing support for response 2...\n",
      "Support assessment: fully supported\n",
      "Step 6: Evaluating utility for response 2...\n",
      "Utility score: 5\n",
      "Generating response for context 3...\n",
      "Step 5: Assessing support for response 3...\n",
      "Support assessment: fully supported\n",
      "Step 6: Evaluating utility for response 3...\n",
      "Utility score: 5\n",
      "Selecting the best response...\n",
      "Best response support: fully supported, utility: 5\n",
      "\n",
      "Final response:\n",
      "areas are particularly vulnerable due to impervious surfaces that prevent water absorption, leading to overwhelmed drainage systems and property damage. Flooding can also result in the contamination of water supplies and the displacement of communities.\n",
      "\n",
      "Ecosystem Disruption\n",
      "Climate change is altering habitats and threatening biodiversity. Species may struggle to adapt to changing temperatures and shifting ecosystems, leading to increased extinction rates. Coral reefs, for example, are suffering from bleaching due to warmer waters, which impacts marine life and the livelihoods of communities that depend on fishing and tourism.\n",
      "\n",
      "Managed Retreat\n",
      "In response to the impacts of climate change, some communities are considering managed retreat, which involves relocating people and infrastructure away from vulnerable areas. This strategy aims to reduce risk and enhance resilience, but it also raises social, economic, and ethical challenges, particularly for those who may be displaced.\n",
      "\n",
      "Overall, the impact of climate change on the environment is profound and multifaceted, affecting weather patterns, ecosystems, and human communities. Addressing these challenges requires comprehensive strategies that include mitigation efforts to reduce greenhouse gas emissions and adaptation measures to enhance resilience.\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the impact of climate change on the environment?\"\n",
    "response = self_rag(query, vectorstore)\n",
    "\n",
    "print(\"\\nFinal response:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093704d2",
   "metadata": {},
   "source": [
    "#### Test the self-RAG function with a more challenging query with low relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28bdb56b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing query: how did harry beat quirrell?\n",
      "Step 1: Determining if retrieval is necessary...\n",
      "Retrieval decision: yes\n",
      "Step 2: Retrieving relevant documents...\n",
      "Retrieved 3 documents\n",
      "Step 3: Evaluating relevance of retrieved documents...\n",
      "Document 1 relevance: irrelevant\n",
      "Document 2 relevance: irrelevant\n",
      "Document 3 relevance: irrelevant\n",
      "Number of relevant contexts: 0\n",
      "No relevant contexts found. Generating without retrieval...\n",
      "\n",
      "Final response:\n",
      "In \"Harry Potter and the Sorcerer's Stone,\" Harry Potter defeats Professor Quirrell, who is possessed by Lord Voldemort, during the climax of the story. Harry confronts Quirrell in the underground chambers where the Sorcerer's Stone is hidden. Quirrell attempts to take the Stone from Harry, but when he touches Harry, he is unable to harm him due to the protection left on Harry by his mother, Lily Potter. This protection is a result of her sacrificial love, which creates a magical barrier against Voldemort. As a result, Quirrell is burned by the touch of Harry, ultimately leading to his defeat.\n"
     ]
    }
   ],
   "source": [
    "query = \"how did harry beat quirrell?\"\n",
    "response = self_rag(query, vectorstore)\n",
    "\n",
    "print(\"\\nFinal response:\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
