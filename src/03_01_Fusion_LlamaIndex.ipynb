{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building an Advanced Fusion Retriever from Scratch\n",
    "\n",
    "In this tutorial, we show you how to build an advanced retriever from scratch.\n",
    "\n",
    "Specifically, we show you how to build our QueryFusionRetriever from scratch.\n",
    "\n",
    "This is heavily inspired from the RAG-fusion repo here: https://github.com/Raudaschl/rag-fusion.\n",
    "\n",
    "#### Setup\n",
    "We load documents and build a simple vector index.\n",
    "\n",
    "1. pip install llama-index-readers-file pymupdf\n",
    "2. pip install llama-index-llms-openai\n",
    "3. pip install llama-index-retrievers-bm25\n",
    "4. pip install llama-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir data\n",
    "!wget --user-agent \"Mozilla\" \"https://arxiv.org/pdf/2307.09288.pdf\" -O \"data/llama2.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from llama_index.readers.file import PyMuPDFReader\n",
    "\n",
    "loader = PyMuPDFReader()\n",
    "documents = loader.load(file_path=\"C:/Praveen/Projects/RAG/data/llama2.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-....\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0.1)\n",
    "embed_model = OpenAIEmbedding(\n",
    "    model=\"text-embedding-3-small\", embed_batch_size=256\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load into Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "splitter = SentenceSplitter(chunk_size=1024)\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, transformations=[splitter], embed_model=embed_model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Advanced Retriever\n",
    "We define an advanced retriever that performs the following steps:\n",
    "\n",
    "1. Query generation/rewriting: generate multiple queries given the original user query\n",
    "2. Perform retrieval for each query over an ensemble of retrievers.\n",
    "3. Reranking/fusion: fuse results from all queries, and apply a reranking step to \"fuse\" the top relevant results!\n",
    "Then in the next section we'll plug this into our response synthesis module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Query Generation/Rewriting\n",
    "The first step is to generate queries from the original query to better match the query intent, and increase precision/recall of the retrieved results. For instance, we might be able to rewrite the query into smaller queries.\n",
    "\n",
    "We can do this by prompting ChatGPT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_str = \"How do the models developed in this work compare to open-source chat models based on the benchmarks tested?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_gen_prompt_str = (\n",
    "    \"You are a helpful assistant that generates multiple search queries based on a \"\n",
    "    \"single input query. Generate {num_queries} search queries, one on each line, \"\n",
    "    \"related to the following input query:\\n\"\n",
    "    \"Query: {query}\\n\"\n",
    "    \"Queries:\\n\"\n",
    ")\n",
    "query_gen_prompt = PromptTemplate(query_gen_prompt_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_queries(llm, query_str: str, num_queries: int = 4):\n",
    "    fmt_prompt = query_gen_prompt.format(\n",
    "        num_queries=num_queries - 1, query=query_str\n",
    "    )\n",
    "    response = llm.complete(fmt_prompt)\n",
    "    queries = response.text.split(\"\\n\")\n",
    "    return queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = generate_queries(llm, query_str, num_queries=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1. Comparison of models developed in this work to open-source chat models in terms of accuracy and efficiency', '2. Evaluation of benchmarks used to test open-source chat models and models developed in this work', '3. Performance analysis of open-source chat models versus models developed in this work on various benchmark datasets']\n"
     ]
    }
   ],
   "source": [
    "print(queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Perform Vector Search for Each Query\n",
    "Now we run retrieval for each query. This means that we fetch the top-k most relevant results from each vector store.\n",
    "\n",
    "NOTE: We can also have multiple retrievers. Then the total number of queries we run is NM, where N is number of retrievers and M is number of generated queries. Hence there will also be NM retrieved lists.\n",
    "\n",
    "Here we'll use the retriever provided from our vector store. If you want to see how to build this from scratch please see [our tutorial on this.](https://docs.llamaindex.ai/en/latest/examples/low_level/retrieval.html#put-this-into-a-retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.asyncio import tqdm\n",
    "\n",
    "\n",
    "async def run_queries(queries, retrievers):\n",
    "    \"\"\"Run queries against retrievers.\"\"\"\n",
    "    tasks = []\n",
    "    for query in queries:\n",
    "        for i, retriever in enumerate(retrievers):\n",
    "            tasks.append(retriever.aretrieve(query))\n",
    "\n",
    "    task_results = await tqdm.gather(*tasks)\n",
    "\n",
    "    results_dict = {}\n",
    "    for i, (query, query_result) in enumerate(zip(queries, task_results)):\n",
    "        results_dict[(query, i)] = query_result\n",
    "\n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get retrievers\n",
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "\n",
    "\n",
    "## vector retriever\n",
    "vector_retriever = index.as_retriever(similarity_top_k=2)\n",
    "\n",
    "## bm25 retriever\n",
    "bm25_retriever = BM25Retriever.from_defaults(\n",
    "    docstore=index.docstore, similarity_top_k=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:02<00:00,  2.30it/s]\n"
     ]
    }
   ],
   "source": [
    "results_dict = await run_queries(queries, [vector_retriever, bm25_retriever])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Perform Fusion\n",
    "The next step here is to perform fusion: combining the results from several retrievers into one and re-ranking.\n",
    "\n",
    "Note that a given node might be retrieved multiple times from different retrievers, so there needs to be a way to de-dup and rerank the node given the multiple retrievals.\n",
    "\n",
    "We'll show you how to perform \"reciprocal rank fusion\": for each node, add up its reciprocal rank in every list where it's retrieved.\n",
    "\n",
    "Then reorder nodes by highest score to least.\n",
    "\n",
    "Full paper here: https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from llama_index.core.schema import NodeWithScore\n",
    "\n",
    "\n",
    "def fuse_results(results_dict, similarity_top_k: int = 2):\n",
    "    \"\"\"Fuse results.\"\"\"\n",
    "    k = 60.0  # `k` is a parameter used to control the impact of outlier rankings.\n",
    "    fused_scores = {}\n",
    "    text_to_node = {}\n",
    "\n",
    "    # compute reciprocal rank scores\n",
    "    for nodes_with_scores in results_dict.values():\n",
    "        for rank, node_with_score in enumerate(\n",
    "            sorted(\n",
    "                nodes_with_scores, key=lambda x: x.score or 0.0, reverse=True\n",
    "            )\n",
    "        ):\n",
    "            text = node_with_score.node.get_content()\n",
    "            text_to_node[text] = node_with_score\n",
    "            if text not in fused_scores:\n",
    "                fused_scores[text] = 0.0\n",
    "            fused_scores[text] += 1.0 / (rank + k)\n",
    "\n",
    "    # sort results\n",
    "    reranked_results = dict(\n",
    "        sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    )\n",
    "\n",
    "    # adjust node scores\n",
    "    reranked_nodes: List[NodeWithScore] = []\n",
    "    for text, score in reranked_results.items():\n",
    "        reranked_nodes.append(text_to_node[text])\n",
    "        reranked_nodes[-1].score = score\n",
    "\n",
    "    return reranked_nodes[:similarity_top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results = fuse_results(results_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.016666666666666666 \n",
      " (2020) and Dinan et al. (2021) also illuminates the difficulties tied to chatbot-oriented\n",
      "LLMs, with concerns ranging from privacy to misleading expertise claims. Deng et al. (2023) proposes\n",
      "a taxonomic framework to tackle these issues, and Bergman et al. (2022) delves into the balance between\n",
      "potential positive and negative impacts from releasing dialogue models.\n",
      "Investigations into red teaming reveal specific challenges in tuned LLMs, with studies by Ganguli et al. (2022)\n",
      "and Zhuo et al. (2023) showcasing a variety of successful attack types and their effects on the generation of\n",
      "harmful content. National security agencies and various researchers, such as (Mialon et al., 2023), have also\n",
      "raised red flags around advanced emergent model behaviors, cyber threats, and potential misuse in areas like\n",
      "biological warfare. Lastly, broader societal issues like job displacement due to accelerated AI research and an\n",
      "over-reliance on LLMs leading to training data degradation are also pertinent considerations (Acemoglu\n",
      "and Restrepo, 2018; Autor and Salomons, 2018; Webb, 2019; Shumailov et al., 2023). We are committed to\n",
      "continuing our work engaging with the broader policy, academic, and industry community on these issues.\n",
      "7\n",
      "Conclusion\n",
      "In this study, we have introduced Llama 2, a new family of pretrained and fine-tuned models with scales\n",
      "of 7 billion to 70 billion parameters. These models have demonstrated their competitiveness with existing\n",
      "open-source chat models, as well as competency that is equivalent to some proprietary models on evaluation\n",
      "sets we examined, although they still lag behind other models like GPT-4. We meticulously elaborated on the\n",
      "methods and techniques applied in achieving our models, with a heavy emphasis on their alignment with the\n",
      "principles of helpfulness and safety. To contribute more significantly to society and foster the pace of research,\n",
      "we have responsibly opened access to Llama 2 and Llama 2-Chat. As part of our ongoing commitment to\n",
      "transparency and safety, we plan to make further improvements to Llama 2-Chat in future work.\n",
      "36 \n",
      "********\n",
      "\n",
      "0.016666666666666666 \n",
      " Model\n",
      "Size\n",
      "GSM8k\n",
      "MATH\n",
      "MPT\n",
      "7B\n",
      "6.8\n",
      "3.0\n",
      "30B\n",
      "15.2\n",
      "3.1\n",
      "Falcon\n",
      "7B\n",
      "6.8\n",
      "2.3\n",
      "40B\n",
      "19.6\n",
      "5.5\n",
      "Llama 1\n",
      "7B\n",
      "11.0\n",
      "2.9\n",
      "13B\n",
      "17.8\n",
      "3.9\n",
      "33B\n",
      "35.6\n",
      "7.1\n",
      "65B\n",
      "50.9\n",
      "10.6\n",
      "Llama 2\n",
      "7B\n",
      "14.6\n",
      "2.5\n",
      "13B\n",
      "28.7\n",
      "3.9\n",
      "34B\n",
      "42.2\n",
      "6.24\n",
      "70B\n",
      "56.8\n",
      "13.5\n",
      "Table 25: Comparison to other open-source models on mathematical reasoning tasks, GSM8k and MATH\n",
      "(maj1@1 is reported).\n",
      "Mathematical Reasoning.\n",
      "In Table 25, we report results for Llama 2 and other open-source datasets on the\n",
      "GSM8k and MATH tasks.\n",
      "A.3\n",
      "Additional Details for Fine-tuning\n",
      "A.3.1\n",
      "Detailed Statistics of Meta Human Preference Data\n",
      "Table 26 shows detailed statistics on Meta human preference data. In total, we collected 14 batches of human\n",
      "preference data (i.e., Meta Safety + Helpfulness) on a weekly basis, consisting of over 1 million binary model\n",
      "generation comparisons. In general, later batches contain more samples as we onboard more annotators over\n",
      "time and the annotators also become more familiar with the tasks and thus have better work efficiency. We\n",
      "also intentionally collect more multi-turn samples to increase the complexity of RLHF data and thus the\n",
      "average number of tokens per sample also increase accordingly over batches.\n",
      "In Figure 25, we plot out the preference rating change over batches. It can be clearly seen that the share\n",
      "of samples with similar responses (e.g., negligibly better or unsure) increase dramatically over time while\n",
      "those with stronger preference (e.g., significantly better) drop in the meantime. This reflects the nature of our\n",
      "iterative model update and preference data annotation procedure - with better-performing Llama 2-Chat\n",
      "models used for response sampling over time, it becomes challenging for annotators to select a better one\n",
      "from two equally high-quality responses.\n",
      "A.3.2\n",
      "Curriculum Strategy for Meta Human Preference Data\n",
      "High quality data is critical for alignment as discussed for SFT. We worked closely with the annotation\n",
      "platforms during our fine-tuning process, and opted for a curriculum annotation strategy. With the first\n",
      "model, the annotators were asked to make prompts relatively simple, and then to progressively move towards\n",
      "more complex prompts and teaching new skills to Llama 2-Chat. An illustration of this curriculum annotation\n",
      "on our helpfulness preference data is displayed in Figure 26.\n",
      "A.3.3\n",
      "Ablation on Ranking Loss with Preference Rating-based Margin for Reward Modeling\n",
      "We ablated the ranking loss with the preference rating-based margin term for the helpfulness reward model.\n",
      "We tried two variants of m(r) with different magnitude for the margin term in Eq 2 as listed open-source 27\n",
      "and compare them against the baseline without the margin term. We report both their per-rating and average\n",
      "accuracy on the Meta Helpful test set in Table 28. We observe that the margin term can indeed help the\n",
      "reward model perform better on more separable comparison pairs and a larger margin can boost it further.\n",
      "However, the larger margin also regresses performance on similar samples.\n",
      "We further evaluated the impact of margin-based loss on reward score distribution shifts. We plot the\n",
      "histogram of reward scores from the test set in Figure 27. Essentially, the margin term pushes the reward\n",
      "51 \n",
      "********\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for n in final_results:\n",
    "    print(n.score, \"\\n\", n.text, \"\\n********\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis**: The above code has a few straightforward components.\n",
    "\n",
    "1. Go through each node in each retrieved list, and add it's reciprocal rank to the node's ID. The node's ID is the hash of it's text for dedup purposes.\n",
    "2. Sort results by highest-score to lowest.\n",
    "3. Adjust node scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plug into RetrieverQueryEngine\n",
    "Now we're ready to define this as a custom retriever, and plug it into our RetrieverQueryEngine (which does retrieval and synthesis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from llama_index.core import QueryBundle\n",
    "from llama_index.core.retrievers import BaseRetriever\n",
    "from llama_index.core.schema import NodeWithScore\n",
    "import asyncio\n",
    "\n",
    "\n",
    "class FusionRetriever(BaseRetriever):\n",
    "    \"\"\"Ensemble retriever with fusion.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        llm,\n",
    "        retrievers: List[BaseRetriever],\n",
    "        similarity_top_k: int = 2,\n",
    "    ) -> None:\n",
    "        \"\"\"Init params.\"\"\"\n",
    "        self._retrievers = retrievers\n",
    "        self._similarity_top_k = similarity_top_k\n",
    "        self._llm = llm\n",
    "        super().__init__()\n",
    "\n",
    "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
    "        \"\"\"Retrieve.\"\"\"\n",
    "        queries = generate_queries(\n",
    "            self._llm, query_bundle.query_str, num_queries=4\n",
    "        )\n",
    "        results = asyncio.run(run_queries(queries, self._retrievers))\n",
    "        final_results = fuse_results(\n",
    "            results, similarity_top_k=self._similarity_top_k\n",
    "        )\n",
    "\n",
    "        return final_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "fusion_retriever = FusionRetriever(\n",
    "    llm, [vector_retriever, bm25_retriever], similarity_top_k=2\n",
    ")\n",
    "\n",
    "query_engine = RetrieverQueryEngine(fusion_retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:01<00:00,  4.00it/s]\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(query_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The models developed in this work demonstrate competitiveness with existing open-source chat models and show competency equivalent to some proprietary models on the evaluation sets examined. However, they still lag behind other models like GPT-4 in terms of performance.\n"
     ]
    }
   ],
   "source": [
    "print(str(response))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
